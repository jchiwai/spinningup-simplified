{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximal Policy Optimization\n",
    "# Original Implementation: https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ppo\n",
    "# Algorithm Doc: https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None: \n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def layers(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers) \n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    '''\n",
    "    Magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    Args: vector x, [x0, x1, x2]\n",
    "    Returns: [x0 + discount * x1 + discount^2 * x2,  x1 + discount * x2, x2]\n",
    "    '''\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        '''\n",
    "        Produces action distribution for a given observation, and\n",
    "        optionally computes log probabilities of given action under these distributions\n",
    "        '''\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "class MLPCategoricalActor(Actor):\n",
    "    '''Categorial actor for discrete action spaces'''\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = layers([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "\n",
    "class MLPGaussianActor(Actor):\n",
    "    '''Gaussian actor for continuous action spaces'''\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32) \n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std)) \n",
    "        self.mu_net = layers([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    '''Critic class'''\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = layers([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    '''Combined Actor-Critic Class'''\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # Actor network depends on action space\n",
    "        if isinstance(action_space, Box):\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)\n",
    "\n",
    "        # Critic network\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, obs):\n",
    "        '''Returns action and value of that action'''\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        '''Returns just the action from step()'''\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    '''\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    '''\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        '''Append one timestep of agent-environment interaction to the buffer.'''\n",
    "        assert self.ptr < self.max_size # Ensures buffer has avaliable space\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        '''\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back over the trajectory and uses rewards \n",
    "        and value estimates to compute advantage estimates and rewards-to-go for each state \n",
    "        to use as the targets for the value function.\n",
    "        '''\n",
    "        path_slice = slice(self.path_start_idx, self.ptr) \n",
    "        rews = np.append(self.rew_buf[path_slice], last_val) # last_val should be 0 if the trajectory has ended, and V(s_T) otherwise\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1] # TD error\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam) # Discounted cumulative sum of TD error (GAE-Lambda advantage calculation)\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1] # Rewards-to-Go\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        '''\n",
    "        Call this at the end of an epoch to get all of the data from the buffer,\n",
    "        with advantages normalized. Also resets some pointers.\n",
    "        '''\n",
    "        assert self.ptr == self.max_size # Ensures buffer is full before collecting\n",
    "        self.ptr, self.path_start_idx = 0, 0 # Reset pointers\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf) # Not using MPI\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std # Normalise advantage (shifted to have mean zero and std one)\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()} # Converts np array to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env_name='HalfCheetah-v4', \n",
    "        actor_critic=MLPActorCritic, \n",
    "        hidden_sizes=[64,64],\n",
    "        steps_per_epoch=5000, \n",
    "        epochs=50, \n",
    "        gamma=0.99,\n",
    "        clip_ratio=0.2, \n",
    "        pi_lr=0.0004, \n",
    "        vf_lr=0.003,\n",
    "        train_pi_iters=80, \n",
    "        train_v_iters=80, \n",
    "        lam=0.97,\n",
    "        target_kl=0.01,\n",
    "        max_ep_length=1000):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "\n",
    "    ac = actor_critic(env.observation_space, env.action_space, hidden_sizes=hidden_sizes) \n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "    print(f'Number of parameters: \\t pi: {var_counts[0]}, \\t v: {var_counts[1]}\\n')\n",
    "\n",
    "    replay_buffer = PPOBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "        _, logp = ac.pi(obs, act) # Under current policy, get the logp of same observations and actions\n",
    "        ratio = torch.exp(logp - logp_old) # Log prob of action under old policy / Log prob of action under current policy\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv # Clips the ratio within 1-e / 1+e\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        return loss_pi, approx_kl\n",
    "\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs) - ret)**2).mean() # Mean squared error loss\n",
    "    \n",
    "    # Optimizers\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update():\n",
    "        data = replay_buffer.get() # Retrieve buffer data\n",
    "\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, kl_loss = compute_loss_pi(data)\n",
    "            if kl_loss > 1.5 * target_kl:\n",
    "                print('Early stopping at step %d due to reaching max KL' %i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "    o, _ = env.reset()  \n",
    "    ep_ret, ep_len = 0, 0\n",
    "    ep_rets = []\n",
    "    epoch_ret = []\n",
    "\n",
    "    # Main experiment loop: collect experience in env and update each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            next_o, r, terminated, _, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "            replay_buffer.store(o, a, r, v, logp)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_length\n",
    "            terminal = terminated or timeout\n",
    "            epoch_ended = t==steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                replay_buffer.finish_path(v)\n",
    "                ep_rets.append(ep_ret)\n",
    "\n",
    "                o, _ = env.reset()  \n",
    "                ep_ret, ep_len = 0, 0\n",
    "    \n",
    "        update()\n",
    "        epoch_ret.append(np.mean(ep_rets))\n",
    "        print('Epoch: %3d \\t Mean epoch return %.3f \\t '% (epoch, epoch_ret[-1]))\n",
    "\n",
    "    return ac, epoch_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "ac, epoch_ret = ppo() # steps_per_epoch=5000, epochs=50 -> 250,000 TotalEnvInteracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "env = gym.make('HalfCheetah-v4', render_mode='rgb_array')\n",
    "o, _ = env.reset() \n",
    "\n",
    "frames = []\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not (terminated or truncated):\n",
    "    frame = env.render()\n",
    "    a, _, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "    next_o, _, terminated, truncated, _  = env.step(a)\n",
    "    o = next_o\n",
    "    frames.append(frame)\n",
    "\n",
    "from PIL import Image\n",
    "def create_gif(frames, filename= 'ppo.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    images[0].save(filename, save_all=True, append_images=images[1:], optimize=False, duration=1, loop=0)\n",
    "\n",
    "create_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
