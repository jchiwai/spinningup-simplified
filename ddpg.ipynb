{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "# https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None: \n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def layers(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers) \n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = layers(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "    \n",
    "class MLPQFunction(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = layers([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "    \n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256), activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    '''A simple FIFO experience replay buffer for DDPG agents.'''\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.terminated_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, terminated):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.terminated_buf[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     terminated=self.terminated_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env_name='HalfCheetah-v4', \n",
    "        actor_critic=MLPActorCritic,\n",
    "        steps_per_epoch=5000, \n",
    "        epochs=50, \n",
    "        replay_size=int(1e6), \n",
    "        gamma=0.99, \n",
    "        polyak=0.995, \n",
    "        pi_lr=0.001, \n",
    "        q_lr=0.001, \n",
    "        batch_size=100, \n",
    "        start_steps=10000, \n",
    "        update_after=1000, \n",
    "        update_every=50, \n",
    "        act_noise=0.1, \n",
    "        num_test_episodes=10, \n",
    "        max_ep_len=1000):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    test_env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "    \n",
    "    ac = actor_critic(env.observation_space, env.action_space)\n",
    "    ac_targ = deepcopy(ac)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q: %d\\n'%var_counts)\n",
    "\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, terminated = data['obs'], data['act'], data['rew'], data['obs2'], data['terminated']\n",
    "        q = ac.q(o,a)\n",
    "        # Bellman backup for Q function\n",
    "        with torch.no_grad():\n",
    "            q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "            backup = r + gamma * (1 - terminated) * q_pi_targ\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q = ((q - backup)**2).mean()\n",
    "        return loss_q\n",
    "    \n",
    "    def compute_loss_pi(data):\n",
    "        o = data['obs']\n",
    "        q_pi = ac.q(o, ac.pi(o))\n",
    "        return -q_pi.mean()\n",
    "    \n",
    "    # Optimizers\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "    def update(data):\n",
    "        # First run one gradient descent step for Q.\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # Freeze Q-network so you don't waste computational effort \n",
    "        # computing gradients for it during the policy learning step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # Next run one gradient descent step for pi.\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "                # Line 15 of Pseudocode\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "    \n",
    "    def test_agent():\n",
    "        ep_rets = []\n",
    "        for _ in range(num_test_episodes):\n",
    "            o, _ = test_env.reset()\n",
    "            terminated = False\n",
    "            ep_ret, ep_len = 0, 0\n",
    "            while not(terminated or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, terminated, _, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            ep_rets.append(ep_ret)\n",
    "        return ep_rets\n",
    "\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    o, _ = env.reset()\n",
    "    epoch_n = 0\n",
    "    ep_len = 0\n",
    "    epoch_ret = []\n",
    "\n",
    "    for t in range(total_steps):\n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy (with some noise, via act_noise). \n",
    "        if t > start_steps:\n",
    "            a = get_action(o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        o2, r, terminated, _, _ = env.step(a)\n",
    "        ep_len += 1\n",
    "        replay_buffer.store(o, a, r, o2, terminated)\n",
    "        o = o2\n",
    "\n",
    "        if terminated or (ep_len==max_ep_len):\n",
    "            o, _ = env.reset()\n",
    "            ep_len = 0\n",
    "\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for _ in range(update_every):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                update(data=batch)\n",
    "\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch_n += 1\n",
    "            ep_rets = test_agent()\n",
    "            epoch_ret.append(np.mean(ep_rets))\n",
    "            print('Epoch: %3d \\t Mean epoch return %.3f \\t '% (epoch_n, epoch_ret[-1]))\n",
    "\n",
    "    return ac, epoch_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "ac, epoch_ret = ddpg() # steps_per_epoch=5000, epochs=50 -> 250,000 TotalEnvInteracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "env = gym.make('HalfCheetah-v4', render_mode='rgb_array')\n",
    "o, _ = env.reset() \n",
    "\n",
    "frames = []\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not (terminated or truncated):\n",
    "    frame = env.render()\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    next_o, _, terminated, truncated, _  = env.step(a)\n",
    "    o = next_o\n",
    "    frames.append(frame)\n",
    "\n",
    "from PIL import Image\n",
    "def create_gif(frames, filename='ddpg.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    images[0].save(filename, save_all=True, append_images=images[1:], optimize=False, duration=1, loop=0)\n",
    "\n",
    "create_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
