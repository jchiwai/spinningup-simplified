{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Actor Critic\n",
    "# https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.optim import Adam\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None: \n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def layers(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers) \n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquashedGaussianMLPActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        self.net = layers([obs_dim] + list(hidden_sizes), activation, activation)\n",
    "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim) \n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
    "        net_out = self.net(obs)\n",
    "\n",
    "        mu = self.mu_layer(net_out)\n",
    "        log_std = self.log_std_layer(net_out)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        pi_distribution = Normal(mu, std)\n",
    "        if deterministic:\n",
    "            # Only used for evaluating policy at test time.\n",
    "            pi_action = mu\n",
    "        else:\n",
    "            pi_action = pi_distribution.rsample()\n",
    "\n",
    "        if with_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding \n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290) \n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "        \n",
    "        pi_action = torch.tanh(pi_action)\n",
    "        pi_action = self.act_limit * pi_action\n",
    "\n",
    "        return pi_action, logp_pi\n",
    "    \n",
    "class MLPQFunction(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = layers([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "    \n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256), activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            a, _ = self.pi(obs, deterministic, False)\n",
    "            return a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    '''A simple FIFO experience replay buffer for SAC agents.'''\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.terminated_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, terminated):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.terminated_buf[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     terminated=self.terminated_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac(env_name='HalfCheetah-v4', \n",
    "        actor_critic=MLPActorCritic,\n",
    "        steps_per_epoch=5000, \n",
    "        epochs=50, \n",
    "        replay_size=int(1e6), \n",
    "        gamma=0.99, \n",
    "        polyak=0.995, \n",
    "        pi_lr=0.001, \n",
    "        q_lr=0.001, \n",
    "        alpha=0.2,\n",
    "        batch_size=100, \n",
    "        start_steps=10000, \n",
    "        update_after=1000, \n",
    "        update_every=50, \n",
    "        num_test_episodes=10, \n",
    "        max_ep_len=1000):\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    test_env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    \n",
    "    ac = actor_critic(env.observation_space, env.action_space)\n",
    "    ac_targ = deepcopy(ac)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # List of parameters for both Q networks\n",
    "    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q1, ac.q2])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d\\n'%var_counts)\n",
    "\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, terminated = data['obs'], data['act'], data['rew'], data['obs2'], data['terminated']\n",
    "\n",
    "        q1 = ac.q1(o,a)\n",
    "        q2 = ac.q2(o,a)\n",
    "        # Bellman backup for Q functions\n",
    "        with torch.no_grad():\n",
    "            # Target actions come from CURRENT policy\n",
    "            a2, logp_a2 = ac.pi(o2)\n",
    "\n",
    "            # Target Q-values\n",
    "            q1_pi_targ = ac_targ.q1(o2, a2)\n",
    "            q2_pi_targ = ac_targ.q2(o2, a2)\n",
    "            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "            backup = r + gamma * (1 - terminated) * (q_pi_targ - alpha * logp_a2)\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q1 = ((q1 - backup)**2).mean()\n",
    "        loss_q2 = ((q2 - backup)**2).mean()\n",
    "        loss_q = loss_q1 + loss_q2\n",
    "        return loss_q\n",
    "\n",
    "    def compute_loss_pi(data):\n",
    "        o = data['obs']\n",
    "        pi, logp_pi = ac.pi(o)\n",
    "        q1_pi = ac.q1(o, pi)\n",
    "        q2_pi = ac.q2(o, pi)\n",
    "        q_pi = torch.min(q1_pi, q2_pi)\n",
    "        loss_pi = (alpha * logp_pi - q_pi).mean() # Entropy-regularized policy loss\n",
    "        return loss_pi\n",
    "    \n",
    "    # Optimizers\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(q_params, lr=q_lr)\n",
    "\n",
    "    def update(data):\n",
    "        # First run one gradient descent step for Q1 and Q2.\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "        \n",
    "        # Freeze Q-network so you don't waste computational effort \n",
    "        # computing gradients for it during the policy learning step.\n",
    "        for p in q_params:\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # Next run one gradient descent step for pi.\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "        for p in q_params:\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "    def get_action(o, deterministic=False):\n",
    "        return ac.act(torch.as_tensor(o, dtype=torch.float32), deterministic)\n",
    "    \n",
    "    def test_agent():\n",
    "        ep_rets = []\n",
    "        for _ in range(num_test_episodes):\n",
    "            o, _ = test_env.reset()\n",
    "            terminated = False\n",
    "            ep_ret, ep_len = 0, 0\n",
    "            while not(terminated or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, terminated, _, _ = test_env.step(get_action(o, True))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            ep_rets.append(ep_ret)\n",
    "        return ep_rets\n",
    "\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    o, _ = env.reset()\n",
    "    epoch_n = 0\n",
    "    ep_len = 0\n",
    "    epoch_ret = []\n",
    "\n",
    "    for t in range(total_steps):\n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy\n",
    "        if t > start_steps:\n",
    "            a = get_action(o)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        o2, r, terminated, _, _ = env.step(a)\n",
    "        ep_len += 1\n",
    "        replay_buffer.store(o, a, r, o2, terminated)\n",
    "        o = o2\n",
    "\n",
    "        if terminated or (ep_len==max_ep_len):\n",
    "            o, _ = env.reset()\n",
    "            ep_len = 0\n",
    "\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for _ in range(update_every):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                update(data=batch)\n",
    "\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch_n += 1\n",
    "            ep_rets = test_agent()\n",
    "            epoch_ret.append(np.mean(ep_rets))\n",
    "            print('Epoch: %3d \\t Mean epoch return %.3f \\t '% (epoch_n, epoch_ret[-1]))\n",
    "\n",
    "    return ac, epoch_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "ac, epoch_ret = sac() # steps_per_epoch=5000, epochs=50 -> 250,000 TotalEnvInteracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "env = gym.make('HalfCheetah-v4', render_mode='rgb_array')\n",
    "o, _ = env.reset() \n",
    "\n",
    "frames = []\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not (terminated or truncated):\n",
    "    frame = env.render()\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32), False)\n",
    "    next_o, _, terminated, truncated, _  = env.step(a)\n",
    "    o = next_o\n",
    "    frames.append(frame)\n",
    "\n",
    "from PIL import Image\n",
    "def create_gif(frames, filename='sac.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    images[0].save(filename, save_all=True, append_images=images[1:], optimize=False, duration=1, loop=0)\n",
    "\n",
    "create_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
